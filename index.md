# Predicting Track Genres using Audio Features
---
## Executive Summary
This project is focused on predicting the genre of a song/track (available on Spotify) based on 10 high-level audio features. We chose to explore this topic since we were interested in finding out if there are characteristics contained within a song (outside of instruments used and societal/cultural influence) that defines the makeup of a genre. We believe this project is especially relevant in today's landscape, since music is ever increasingly pushing boundaries in terms of stylistic choices. We were keen to explore whether a supervised machine learning approach could help answer the question and further help us infer if there are any other genres influencing the sound of a song.

For this project, we collated over 7,000 songs belonging to 9 different genres which were collected from automatically-generated playlists found on [Every Noise at Once](https://everynoise.com/). We manually collected and tagged the relevant playlist ID's and then used Spotify's Web APIs to retrieve the relevant data for preprocessing and analysis. With the relevant preprocessing and exploratory data analysis complete, we had a final dataset of over 6,500 songs each containing 11 high-level audio features.

With our dataset at hand, we began exploring machine learning techniques that could be used for this project. We explored a handful of techniques (even some within the deep learning domain) and ultimately decided on using a Label Powerset approach with a Logistic Regression base classifier. Using these techniques, we found that our model could predict the genre(s) of a song with 45-50% full accuracy on average. We also discovered some interesting findings related to the relationships between certain genres and the kind of songs that have multiple genres tagged to them. We concluded that our model was partially successful in being able to predict the genres of songs based on audio features, since the model performs well in terms of partial accuracy, and does indeed identify other genres as possible outputs (something which we will discuss in greater depth in the coming sections). We also made our final model available online as a web app over at the (aptly named) [Track Genre Predictor ðŸŽµ](https://ds105.herokuapp.com/).

![preview of genre predictor](https://raw.githubusercontent.com/iuven1s/ds105-project/main/genre_predictor_img.png)

## Motivation and Justification
Classifying music based on a signature 'sound' or 'feel' (i.e. a genre) is a surprisingly difficult task; whether we are aware of it or not, our favourite songs are influenced by the sound and feel of songs dating decades back (indeed, one can trace the roots of EDM all the way back to rhythm and blues!) It is the difficulty of this task which motivated us to explore if it was possible to predict the genre of any given song based on the characteristics of the song. Of course, there are more variables to consider than just the audio features (e.g. use of certain instruments, lyrics, societal/cultural influences), but for our quantitative model, we thought it would be suitable to use the audio features retrieved from Spotify. We believe this project justified since music is becoming increasingly complex and diverse, and with it comes confusion in classifying within genres. Indeed, in some cases, there are so many songs that fit more than one genre that they warrant the existence of an entirely new genre which blends the genres together (a subgenre). Thus, it would be apt to first see if we can find the base genres and whether our model can successfully determine them based on the audio features alone.

## Aim
With this project, we aim to do two things. Firstly, we aim to see how successful a machine learning model is at predicting the genres associated with a track. Secondly, we would like to see if our model can

## Data
As mentioned earlier, our primary data source is Every Noise at Once. We manually retrieved 9 playlist ID's corresponding to the main 9 genres that we were going to work with. These were classical, country, EDM, hip-hop, jazz, pop, rap, RnB, and rock. Using these playlist ID's, we iteratively retrieved the ID's of each song contained within a playlist by interacting with Spotify's Web API endpoints (using helper functions we built ourselves). After we had all the track ID's, we used another endpoint to retrieve the 13 audio features associated with each track (danceability, energy, acousticness, liveness, valence, tempo, time signature, loudness, key, mode, instrumentalness, speechiness, and duration in milliseconds). The description of these features can be seen below:


| **Feature**      | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Acousticness     | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.                                                                                                                                                                                                                                                                                                                                                  |
| Danceability     | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.                                                                                                                                                                                                                                  |
| Duration         | The duration of the track in milliseconds.                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Energy           | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.                                                                                                                                                                             |
| Instrumentalness | Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.                                                                            |
| Key              | The key the track is in. Integers map to pitches using standard Pitch Class notation. If no key was detected, the value is -1. (Range [-1, 11]).                                                                                                                                                                                                                                                                                                                              |
| Liveness         | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.                                                                                                                                                                                                                                                       |
| Loudness         | The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. (Typical range [-60, 0]).                                                                                                                                                                                                                                                                                    |
| Mode             | Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.                                                                                                                                                                                                                                                                                                               |
| Speechiness      | Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording, the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |
| Tempo            | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.                                                                                                                                                                                                                                                                                    |
| Time signature   | An estimated time signature. The time signature is a notational convention to specify how many beats are in each bar. The time signature ranges from 3 to 7 including time signatures of "3/4", to "7/4".                                                                                                                                                                                                                                                                     |
| Valence          | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive, while tracks with low valence sound more negative.                                                                                                                                                                                                                                                                                           |


Once we had all the audio features, we merged everything together and stored our final dataset in one pandas dataframe which we also serialised for local storage. This dataset initially contained 7,451 tracks with information on the ID, artist(s), name, genre, and 13 audio features. Summary statistics can be seen below: