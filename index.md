# Predicting Track Genres using Audio Features
---
## Executive Summary
This project is focused on predicting the genre of a song/track (available on Spotify) based on 10 high-level audio features. We chose to explore this topic since we were interested in finding out if there are characteristics contained within a song (outside of instruments used and societal/cultural influence) that defines the makeup of a genre. We believe this project is especially relevant in today's landscape, since music is ever increasingly pushing boundaries in terms of stylistic choices. We were keen to explore whether a supervised machine learning approach could help answer the question and further help us infer if there are any other genres influencing the sound of a song.

For this project, we collated over 7,000 songs belonging to 9 different genres which were collected from automatically-generated playlists found on [Every Noise at Once](https://everynoise.com). We manually collected and tagged the relevant playlist ID's and then used Spotify's Web APIs to retrieve the relevant data for preprocessing and analysis. With the relevant preprocessing and exploratory data analysis complete, we had a final dataset of over 6,500 songs each containing 11 high-level audio features.

With our dataset at hand, we began exploring machine learning techniques that could be used for this project. We explored a handful of techniques (even some within the deep learning domain) and ultimately decided on using a Label Powerset approach with a Logistic Regression base classifier. Using these techniques, we found that our model could predict the genre(s) of a song with 45-50% full accuracy on average. We also discovered some interesting findings related to the relationships between certain genres and the kind of songs that have multiple genres tagged to them. We concluded that our model was partially successful in being able to predict the genres of songs based on audio features, since the model performs well in terms of partial accuracy, and does indeed identify other genres as possible outputs (something which we will discuss in greater depth in the coming sections). We also made our final model available online as a web app over at the (aptly named) [Track Genre Predictor ðŸŽµ](https://ds105.herokuapp.com/).

![preview of genre predictor](https://raw.githubusercontent.com/iuven1s/ds105-project/main/genre_predictor_img.png)

## Motivation and Justification
Classifying music based on a signature 'sound' or 'feel' (i.e. a genre) is a surprisingly difficult task; whether we are aware of it or not, our favourite songs are influenced by the sound and feel of songs dating decades back (indeed, one can trace the roots of EDM all the way back to rhythm and blues!) It is the difficulty of this task which motivated us to explore if it was possible to predict the genre of any given song based on the characteristics of the song. Of course, there are more variables to consider than just the audio features (e.g. use of certain instruments, lyrics, societal/cultural influences), but for our quantitative model, we thought it would be suitable to use the audio features retrieved from Spotify. We believe this project justified since music is becoming increasingly complex and diverse, and with it comes confusion in classifying within genres. Indeed, in some cases, there are so many songs that fit more than one genre that they warrant the existence of an entirely new genre which blends the genres together (a subgenre). Thus, it would be apt to first see if we can find the base genres and whether our model can successfully determine them based on the audio features alone.

## Aim
With this project, we aim to do two things. Firstly, we aim to see how successful a machine learning model is at predicting the genres associated with a track. Secondly, we would like to see what relationships our model picks up between genres (such that we may observe what genres, if any, may be influencing a track).

## Data
As mentioned earlier, our primary data source is Every Noise at Once. We manually retrieved 9 playlist ID's corresponding to the main 9 genres that we were going to work with. These were classical, country, EDM, hip-hop, jazz, pop, rap, RnB, and rock. Using these playlist ID's, we iteratively retrieved the ID's of each song contained within a playlist by interacting with Spotify's Web API endpoints (using helper functions we built ourselves). After we had all the track ID's, we used another endpoint to retrieve the 13 audio features associated with each track (danceability, energy, acousticness, liveness, valence, tempo, time signature, loudness, key, mode, instrumentalness, speechiness, and duration in milliseconds). The description of these features can be seen below:


| **Feature**      | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Acousticness     | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.                                                                                                                                                                                                                                                                                                                                                  |
| Danceability     | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.                                                                                                                                                                                                                                  |
| Duration         | The duration of the track in milliseconds.                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Energy           | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.                                                                                                                                                                             |
| Instrumentalness | Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.                                                                            |
| Key              | The key the track is in. Integers map to pitches using standard Pitch Class notation. If no key was detected, the value is -1. (Range [-1, 11]).                                                                                                                                                                                                                                                                                                                              |
| Liveness         | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.                                                                                                                                                                                                                                                       |
| Loudness         | The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. (Typical range [-60, 0]).                                                                                                                                                                                                                                                                                    |
| Mode             | Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.                                                                                                                                                                                                                                                                                                               |
| Speechiness      | Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording, the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |
| Tempo            | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.                                                                                                                                                                                                                                                                                    |
| Time signature   | An estimated time signature. The time signature is a notational convention to specify how many beats are in each bar. The time signature ranges from 3 to 7 including time signatures of "3/4", to "7/4".                                                                                                                                                                                                                                                                     |
| Valence          | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive, while tracks with low valence sound more negative.                                                                                                                                                                                                                                                                                           |


Once we had all the audio features, we merged everything together and stored our final dataset in one pandas dataframe which we also serialised for local storage. This dataset initially contained 7,451 tracks with information on the ID, artist(s), name, genre, and 13 audio features. Summary statistics can be seen below:


![barplot showing count of all genres](https://raw.githubusercontent.com/iuven1s/ds105-project/main/all_genres.png)


|                      | Danceability | Energy   | Key       | Loudness   | Mode     | Speechiness | Acousticness | Instrumentalness | Liveness | Valence  | Tempo      | Duration (ms) | Time Signature |
|----------------------|--------------|----------|-----------|------------|----------|-------------|--------------|------------------|----------|----------|------------|---------------|----------------|
| Count                | 7451         | 7451     | 7451      | 7451       | 7451     | 7451        | 7451         | 7451             | 7451     | 7451     | 7451       | 7451          | 7451           |
| Mean                 | 0.615136     | 0.612725 | 5.289894  | -8.438428  | 0.605422 | 0.101815    | 0.259658     | 0.100764         | 0.179561 | 0.489036 | 118.203124 | 238294.7      | 3.937726       |
| Standard Deviation   | 0.176619     | 0.237958 | 3.565755  | 5.456235   | 0.488793 | 0.094426    | 0.309558     | 0.252381         | 0.142371 | 0.242284 | 28.609772  | 76054.92      | 0.340766       |
| Minimum value        | 0.061000     | 0.000885 | 0.000000  | -43.738000 | 0.000000 | 0.022400    | 0.000003     | 0.000000         | 0.013600 | 0.023700 | 42.313000  | 120133.0      | 1.000000       |
| First quartile (25%) | 0.507000     | 0.479000 | 2.000000  | -9.809000  | 0.000000 | 0.037900    | 0.023700     | 0.000000         | 0.092700 | 0.300000 | 95.220500  | 192000.0      | 4.000000       |
| Median (50%)         | 0.634000     | 0.654000 | 5.000000  | -6.808000  | 1.000000 | 0.054100    | 0.116000     | 0.000007         | 0.122000 | 0.494000 | 118.039000 | 223847.0      | 4.000000       |
| Third quartile (75%) | 0.743000     | 0.797000 | 8.000000  | -5.080500  | 1.000000 | 0.133500    | 0.394000     | 0.004025         | 0.226000 | 0.682000 | 134.799000 | 264723.0      | 4.000000       |
| Maximum value        | 0.983000     | 0.998000 | 11.000000 | 0.915000   | 1.000000 | 0.400000    | 0.996000     | 0.982000         | 0.979000 | 0.985000 | 210.857000 | 1148947.0     | 5.000000       |


From the above, we can see a few interesting observations. Firstly, no songs in this database are exclusively speech-like (which makes sense given they are meant to be songs), as the maximum value for speechiness is 0.4, which is in the 0.33-0.66 range of music/speech mix according to the feature description. Most songs are also not acoustic (which also makes sense given the prevalence of producing music digitally, and given the high proportion of rock and pop music in this dataset). Most songs also do contain vocals, as indicated by the low mean/median values for instrumentalness. Most songs also tend to be 'neutral' in terms of musical positivity, as the mean and median values are both quite close to 0.5.

## Methodology
After finding the basic summary statistics, we moved on to preprocessing for the model. Our preprocessing stage was iterative (in the sense that we preprocessed many times throughout the course of our project). During an earlier preprocessing stage, we made two major changes to the dataset. Firstly, we normalised all the values using min-max scaling (through scikit-learn's `preprocessing` modules) so that all the values were between 0 and 1. We did this as some machine learning techniques require the data to be normalised, and at this stage we had not decided which technique we were going to use, so we scaled all the data just in case. Secondly, we noticed that there were many duplicate track ID's in the dataset. Initially, we chose to drop all duplicate values except the first instance, since at this stage we were still treating this as a single output problem. Having preprocessed the data, we moved on to creating a model. Initially, we went for a decision tree, as it is a relatively simple method and is easy to visualise. We manually conducted a one-vs-all approach for all 9 genres, complete with minimal cost-complexity pruning to optimise the model and ensure we were not overfitting. In the end, we noted that the model was able to guess a track's genre accurately around 80% of the time (with the model having the most success with classical music at 93% and the most trouble with rap at 69%).

After this model, we thought more carefully about our dataset. The most troubling limitation with our data at this stage was that we didn't know how to sufficiently deal with the duplicate values (i.e. the songs which had more than one genre tagged to them). However, after conducting some more research, we shifted our focus and realised that we would need to incorporate these multi-genre songs into the model. For this reason, we turned to multilabel solutions (using scikit-learn and scikit-multilearn). At this stage, we decided to preprocess our data again. Instead of dropping duplicate tracks, we instead gave each track in our dataframe an array of binary values representing genres, with 1 specifying that the song belongs to that genre (based on the index), and 0 specifying the oppposite. That way, songs that are, for example, both pop and rock could have an array like `[0, 0, 0, 0, 0, 1, 0, 0, 1]`, where the 6th and 9th index signify pop and rock. During this preprocessing stage, we also conducted some basic exploratory data analysis again, noting that some features were collinear. Most notably, loudness and energy were very strongly positively correlated (0.81) and both acousticness/energy and acousticness/loudness were strongly negatively correlated (-0.79 and -0.71 respectively). At this stage, we were considering a logistic regression for our model, and so having collinearity in our independent variables is less than ideal. For this reason, we chose to discard both acousticness and energy. We specifically chose this combination over any other because we judged loudness to be the most important variable out of the three; both acousticness and energy are predicted values from Spotify's own proprietary algorithms which we have no access to. However, loudness is an empirically measured value that can be observed from the track's properties. Hence, we decided to keep the most empirically accurate variable.

![correlation matrix for audio features](https://raw.githubusercontent.com/iuven1s/ds105-project/main/features_correlation.png)

After preprocessing our data, we used a Label Powerset approach with a Logistic Regression base classifier using scikit-multilearn and scikit-learn. Essentially, the Label Powerset approach transforms our multilabel problem into a multiclass problem such that we can iteratively apply a logistic regression for each song against each genre. Once again, we used an 80/20 split for our training and testing data and trained our model. We found that, on average, the model performs with around 45-50% accuracy (using scikit-learn's `accuracy_score` metric).

## Results
From both our decision tree and logistic regression approaches, we learned many things about the relationship between the audio features of a track and its genre, as well as the relationships between genres in general.

Firstly, with the decision tree, we were able to interpret feature importance in the model and find which variables were most important when determining the genre of a track. Often, the features were characteristic of that genre in general, meaning the model was able to pick up on important features successfully. For example, classical music mostly had low loudness and low danceability as its most important features. EDM's most important variables were high energy and high tempo, and pop was typically short in duration and loud. Some of the other genre's decision trees were a little harder to interpet, as the depth of the trees were deeper, but nonetheless all trees generally seemed to follow a pattern of feature importance which made sense for the genre.

| **Genre** | **Accuracy** |
|-----------|--------------|
| Classical | 93%          |
| Country   | 84%          |
| EDM       | 79%          |
| Hip-hop   | 85%          |
| Jazz      | 90%          |
| Pop       | 71%          |
| Rap       | 69%          |
| RnB       | 73%          |
| Rock      | 75%          |

We suspect the above accuracy values are due to how distinct these genres are from one another in terms of audio features. For example, the class of classical music as a whole tends to be more uniform in its audio features compared to, say, EDM (which, in real life, has many subgenres); the standard deviation for some of the important features are, on the whole, lower for classical music than for EDM (see danceability, energy and speechiness). Loudness is a little bit harder to interpret because dB is a logarithmic scale; classical music is quieter (-23 dB average compared to -6 dB for EDM) and the scale is more sensitive at this end of the scale, so a higher standard deviation is expected.

| **Genre** | **Danceability** | **Energy** | **Key**  | **Loudness** | **Mode** | **Speechiness** | **Acousticness** | **Instrumentalness** | **Liveness** | **Valence** | **Tempo** | **Duration** | **Time signature** |
|-----------|------------------|------------|----------|--------------|----------|-----------------|------------------|----------------------|--------------|-------------|-----------|--------------|--------------------|
| Classical | 0.131175         | 0.096639   | 3.403792 | 6.190403     | 0.471951 | 0.015401        | 0.078896         | 0.359251             | 0.082796     | 0.160197    | 29.417205 | 164007.7     | 0.719302           |
| EDM       | 0.126013         | 0.122626   | 3.638411 | 1.948147     | 0.500219 | 0.062831        | 0.113523         | 0.273418             | 0.150812     | 0.214103    | 18.159212 | 47162.902561 | 0.215304           |

<p align="center">Standard deviation for Classical and EDM</p>

From the 

## Conclusion
Lorem ipsum

## Appendix
- [Spotify Web API Documentation](https://developer.spotify.com/documentation/web-api/reference)
- [Every Noise at Once](https://everynoise.com)
- [Track Genre Predictor Web App](https://ds105.herokuapp.com)
- [GitHub repository with source code and data](https://github.com/iuven1s/ds105-project)