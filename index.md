# Predicting Track Genres using Audio Features
---
## Executive Summary
This project is focused on predicting the genre of a song/track (available on Spotify) based on 10 high-level audio features. We chose to explore this topic since we were interested in finding out if there are characteristics contained within a song (outside of instruments used and societal/cultural influence) that defines the makeup of a genre. We believe this project is especially relevant in today's landscape, since music is ever increasingly pushing boundaries in terms of stylistic choices. We were keen to explore whether a supervised machine learning approach could help answer the question and further help us infer if there are any other genres influencing the sound of a song.

For this project, we collated over 7,000 songs belonging to 9 different genres which were collected from automatically-generated playlists found on [Every Noise at Once](https://everynoise.com/). We manually collected and tagged the relevant playlist ID's and then used Spotify's Web APIs to retrieve the relevant data for preprocessing and analysis. With the relevant preprocessing and exploratory data analysis complete, we had a final dataset of over 6,500 songs each containing 11 high-level audio features.

With our dataset at hand, we began exploring machine learning techniques that could be used for this project. We explored a handful of techniques (even some within the deep learning domain) and ultimately decided on using a Label Powerset approach with a Logistic Regression base classifier. Using these techniques, we found that our model could predict the genre(s) of a song with 45-50% full accuracy on average. We also discovered some interesting findings related to the relationships between certain genres and the kind of songs that have multiple genres tagged to them. We concluded that our model was partially successful in being able to predict the genres of songs based on audio features, since the model performs well in terms of partial accuracy, and does indeed identify other genres as possible outputs (something which we will discuss in greater depth in the coming sections). We also made our final model available online as a web app over at the (aptly named) [Track Genre Predictor ðŸŽµ](https://ds105.herokuapp.com/).

![preview of genre predictor](https://raw.githubusercontent.com/iuven1s/ds105-project/main/genre_predictor_img.png)

## Motivation and Justification
Classifying music based on a signature 'sound' or 'feel' (i.e. a genre) is a surprisingly difficult task; whether we are aware of it or not, our favourite songs are influenced by the sound and feel of songs dating decades back (indeed, one can trace the roots of EDM all the way back to rhythm and blues!) It is the difficulty of this task which motivated us to explore if it was possible to predict the genre of any given song based on the characteristics of the song. Of course, there are more variables to consider than just the audio features (e.g. use of certain instruments, lyrics, societal/cultural influences), but for our quantitative model, we thought it would be suitable to use the audio features retrieved from Spotify. We believe this project justified since music is becoming increasingly complex and diverse, and with it comes confusion in classifying within genres. Indeed, in some cases, there are so many songs that fit more than one genre that they warrant the existence of an entirely new genre which blends the genres together (a subgenre). Thus, it would be apt to first see if we can find the base genres and whether our model can successfully determine them based on the audio features alone.

## Aim
With this project, we aim to do two things. Firstly, we aim to see how successful a machine learning model is at predicting the genres associated with a track. Secondly, we would like to see what relationships our model picks up between genres (such that we may observe what genres, if any, may be influencing a track).

## Data
As mentioned earlier, our primary data source is Every Noise at Once. We manually retrieved 9 playlist ID's corresponding to the main 9 genres that we were going to work with. These were classical, country, EDM, hip-hop, jazz, pop, rap, RnB, and rock. Using these playlist ID's, we iteratively retrieved the ID's of each song contained within a playlist by interacting with Spotify's Web API endpoints (using helper functions we built ourselves). After we had all the track ID's, we used another endpoint to retrieve the 13 audio features associated with each track (danceability, energy, acousticness, liveness, valence, tempo, time signature, loudness, key, mode, instrumentalness, speechiness, and duration in milliseconds). The description of these features can be seen below:


| **Feature**      | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Acousticness     | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.                                                                                                                                                                                                                                                                                                                                                  |
| Danceability     | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.                                                                                                                                                                                                                                  |
| Duration         | The duration of the track in milliseconds.                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Energy           | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.                                                                                                                                                                             |
| Instrumentalness | Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.                                                                            |
| Key              | The key the track is in. Integers map to pitches using standard Pitch Class notation. If no key was detected, the value is -1. (Range [-1, 11]).                                                                                                                                                                                                                                                                                                                              |
| Liveness         | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.                                                                                                                                                                                                                                                       |
| Loudness         | The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. (Typical range [-60, 0]).                                                                                                                                                                                                                                                                                    |
| Mode             | Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.                                                                                                                                                                                                                                                                                                               |
| Speechiness      | Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording, the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |
| Tempo            | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.                                                                                                                                                                                                                                                                                    |
| Time signature   | An estimated time signature. The time signature is a notational convention to specify how many beats are in each bar. The time signature ranges from 3 to 7 including time signatures of "3/4", to "7/4".                                                                                                                                                                                                                                                                     |
| Valence          | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive, while tracks with low valence sound more negative.                                                                                                                                                                                                                                                                                           |


Once we had all the audio features, we merged everything together and stored our final dataset in one pandas dataframe which we also serialised for local storage. This dataset initially contained 7,451 tracks with information on the ID, artist(s), name, genre, and 13 audio features. Summary statistics can be seen below:


![barplot showing count of all genres](https://raw.githubusercontent.com/iuven1s/ds105-project/main/all_genres.png)


|                      | Danceability | Energy   | Key       | Loudness   | Mode     | Speechiness | Acousticness | Instrumentalness | Liveness | Valence  | Tempo      | Duration (ms) | Time Signature |
|----------------------|--------------|----------|-----------|------------|----------|-------------|--------------|------------------|----------|----------|------------|---------------|----------------|
| Count                | 7451         | 7451     | 7451      | 7451       | 7451     | 7451        | 7451         | 7451             | 7451     | 7451     | 7451       | 7451          | 7451           |
| Mean                 | 0.615136     | 0.612725 | 5.289894  | -8.438428  | 0.605422 | 0.101815    | 0.259658     | 0.100764         | 0.179561 | 0.489036 | 118.203124 | 238294.7      | 3.937726       |
| Standard Deviation   | 0.176619     | 0.237958 | 3.565755  | 5.456235   | 0.488793 | 0.094426    | 0.309558     | 0.252381         | 0.142371 | 0.242284 | 28.609772  | 76054.92      | 0.340766       |
| Minimum value        | 0.061000     | 0.000885 | 0.000000  | -43.738000 | 0.000000 | 0.022400    | 0.000003     | 0.000000         | 0.013600 | 0.023700 | 42.313000  | 120133.0      | 1.000000       |
| First quartile (25%) | 0.507000     | 0.479000 | 2.000000  | -9.809000  | 0.000000 | 0.037900    | 0.023700     | 0.000000         | 0.092700 | 0.300000 | 95.220500  | 192000.0      | 4.000000       |
| Median (50%)         | 0.634000     | 0.654000 | 5.000000  | -6.808000  | 1.000000 | 0.054100    | 0.116000     | 0.000007         | 0.122000 | 0.494000 | 118.039000 | 223847.0      | 4.000000       |
| Third quartile (75%) | 0.743000     | 0.797000 | 8.000000  | -5.080500  | 1.000000 | 0.133500    | 0.394000     | 0.004025         | 0.226000 | 0.682000 | 134.799000 | 264723.0      | 4.000000       |
| Maximum value        | 0.983000     | 0.998000 | 11.000000 | 0.915000   | 1.000000 | 0.400000    | 0.996000     | 0.982000         | 0.979000 | 0.985000 | 210.857000 | 1148947.0     | 5.000000       |


From the above, we can see a few interesting observations. Firstly, it seems to be the case that across the dataset, most songs tend to be danceable and energetic. Most songs are also loud, with the median value around -6.8 dB (for comparison, the loudest track is 0.915 dB and the quietest is -43.738 dB). Also, according to Spotify's analysis, most songs are in major (at least 3/4). No songs in this database are exclusively speech-like (which makes sense), as the maximum value for speechiness is 0.4, which is in the 0.33-0.66 range of music/speech mix. Most songs are also not acoustic (which also makes sense given the prevalence of producing music digitally, and given the high proportion of rock and pop music in this dataset). Most songs also do contain vocals, as indicated by the low mean/median values for instrumentalness. Most songs also tend to be 'neutral' in terms of musical positivity, as the mean and median values are both quite close to 0.5.

## Methodology
After finding the basic summary statistics, we moved on to preprocessing for the model. During our preprocessing stage, we made three major changes to the dataset. Firstly, we normalised all the values using min-max scaling so that all the values were between 0 and 1. We did this as some machine learning techniques require the data to be normalised, and at this stage we had not decided which technique we were going to use, so we scaled all the data just in case. Secondly, we noticed that there were many duplicate track ID's in the dataset. Initially, we chose to drop all duplicate values except the first instance, since at this stage we were still treating this as a single output problem. Thirdly, having done some basic exploratory data analysis, we spotted that some features were collinear.

